apiVersion: v1
kind: ConfigMap
metadata:
  name: spark-configs
  namespace: spark-k8s
  labels:
    app.kubernetes.io/name: spark
data:
  spark-defaults.conf: |
    spark.hadoop.fs.s3a.aws.credentials.provider                com.amazonaws.auth.EnvironmentVariableCredentialsProvider
    spark.hadoop.fs.s3a.endpoint                                minio-service.spark-k8s:9000
    spark.hadoop.fs.s3a.path.style.access                       true
    spark.hadoop.fs.s3a.connection.ssl.enabled                  false
    spark.driver.extraJavaOptions                               --add-exports=java.base/sun.nio.ch=ALL-UNNAMED
    spark.executor.extraJavaOptions                             --add-exports=java.base/sun.nio.ch=ALL-UNNAMED
    spark.kubernetes.container.image                            maluz/custom-spark:3.5.3
    spark.kubernetes.driver.podTemplateFile                     s3a://pod-templates/spark-pod-template.yml
    spark.kubernetes.executor.podTemplateFile                   s3a://pod-templates/spark-pod-template.yml
    spark.kubernetes.authenticate.driver.serviceAccountName     spark-service-account
    spark.kubernetes.authenticate.executor.serviceAccountName   spark-service-account
    spark.kubernetes.namespace                                  spark-k8s

---
apiVersion: batch/v1
kind: Job
metadata:
  name: run-spark-task
  namespace: spark-k8s
  labels:
    app.kubernetes.io/name: spark
spec:
  template:
    metadata:
      name: run-spark-task-pod
      namespace: spark-k8s
      labels:
        app.kubernetes.io/name: spark
    spec:
      serviceAccountName: spark-service-account
      containers:
        - name: run-spark-task
          image: maluz/custom-spark:3.5.3
          args:
            - "/opt/spark/bin/spark-submit"
            - "--master"
            - "k8s://https://192.168.49.2:8443"
            - "--deploy-mode"
            - "cluster"
            - "--name"
            - "spark-example-task"
            - "--class"
            - "it.unimi.SparkSample"
            - "--conf"
            - "spark.executor.instances=1"
            - "s3a://jobs/sparksample.jar"
            - "s3a://datasets/d1.csv"
            - "s3a://output/sparksample-output"
          env:
            - name: AWS_ACCESS_KEY_ID
              valueFrom:
                secretKeyRef:
                  name: minio-credentials
                  key: MINIO_ROOT_USER
            - name: AWS_SECRET_ACCESS_KEY
              valueFrom:
                secretKeyRef:
                  name: minio-credentials
                  key: MINIO_ROOT_PASSWORD
          volumeMounts:
            - name: spark-config-volume
              mountPath: /opt/spark/conf
              readOnly: true
      volumes:
        - name: spark-config-volume
          configMap:
            name: spark-configs
            items:
              - key: "spark-defaults.conf"
                path: "spark-defaults.conf"
      restartPolicy: Never
  backoffLimit: 1
